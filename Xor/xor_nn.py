# -*- coding: utf-8 -*-
"""XOR nn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L-FYSxIeA7V9GlP7YatotfXVCYpFChGe
"""

import numpy as np

#XOR Gate
X=np.array([[0,0],[0,1],[1,0],[1,1]])
Y=np.array([[0],[1],[1],[0]])

def sigmoid(x):
  return 1/(1+np.exp(-x))

input_layer=2
hidden=2
output_layer=1
lr=0.1
weights={
    'h1':2*np.random.random((input_layer,hidden))-1,
    'out':2*np.random.random((hidden,output_layer))-1
}
bias={
    'b1':2*np.random.random((1,hidden))-1,
    'bo':2*np.random.random((1,output_layer))-1
}

def forward_propagation(x,weight,bias):
  x=np.matmul(x,weight['h1'])+bias['b1']
  x=sigmoid(x)
  x=np.matmul(x,weight['out'])+bias['bo']
  x=sigmoid(x)
  return x

def backward_propagation(pred,y):
  gra1=pred-y
  gra1=gra1*pred*(1-pred)
  hidd=np.matmul(gra1,weights['out'].T)
  temp=np.matmul(X,weights['h1'])+bias['b1']
  temp=sigmoid(temp)
  hidd*=(temp*(1-temp))
  changes_hidden=np.matmul(X.T,hidd)
  bias_hidden=np.sum(hidd,axis=0)
  bias['b1']=bias['b1']-lr*bias_hidden
  weights['h1']=weights['h1']-lr*changes_hidden
  changes_output=np.matmul(temp.T,gra1)
  bias_output=np.sum(gra1,axis=0)
  weights['out']=weights['out']-lr*changes_output
  bias['bo']=bias['bo']-lr*bias_output

for i in range(100000):
  pred=forward_propagation(X,weights,bias)
  backward_propagation(pred,Y)
pred=forward_propagation(X,weights,bias)
print(pred)

weights

bias